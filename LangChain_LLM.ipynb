{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "5PmseQsMmdYY",
    "outputId": "8f3828e1-cb93-4ca7-d6cb-b329965a16c4"
   },
   "outputs": [],
   "source": [
    "# !pip install openai\n",
    "# !pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "tMzhJ24Jmi9i"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "import datetime\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "en7RsF9Um2dE"
   },
   "outputs": [],
   "source": [
    "# f = open(\"/content/sample_data/OPENAI_API_KEY.txt\", \"r\")\n",
    "f = open(\"OPENAI_API_KEY.txt\", \"r\")\n",
    "openai_api_key = f.read()\n",
    "# print(openai_api_key)\n",
    "client = openai.OpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "_TyLogXKm8zS"
   },
   "outputs": [],
   "source": [
    "llm_model = \"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Zrd3QOE9nFNB"
   },
   "outputs": [],
   "source": [
    "def get_completion_and_token_count(prompt,\n",
    "                                   model=llm_model,\n",
    "                                   temperature=0,\n",
    "                                   max_tokens=500):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt+' output in JSON format'}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,        # messages = [{\"role\": \"user\", \"content\": prompt}], a list of messages comprising the conversation so far.\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,    # The maximum number of tokens that can be 'generated' by the chat completion.\n",
    "    )\n",
    "\n",
    "    content = response.choices[0].message.content\n",
    "\n",
    "    token_dict = {\n",
    "        'prompt_tokens':response.usage.prompt_tokens, # no. of input tokens counted by OpenAI API\n",
    "        'completion_tokens':response.usage.completion_tokens,\n",
    "        'total_tokens':response.usage.total_tokens,\n",
    "    }\n",
    "    print(response)\n",
    "    print(content)\n",
    "    print(token_dict)\n",
    "\n",
    "    return response, content, token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "G3Syhi2QnHxG",
    "outputId": "2f381f34-5466-4912-df98-90359715fa41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-936xiynhdgEgMfYCEjLNdQJB3uRWM', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='{\\n  \"result\": 2\\n}', role='assistant', function_call=None, tool_calls=None))], created=1710528774, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_4f2ebda25a', usage=CompletionUsage(completion_tokens=9, prompt_tokens=18, total_tokens=27))\n",
      "{\n",
      "  \"result\": 2\n",
      "}\n",
      "{'prompt_tokens': 18, 'completion_tokens': 9, 'total_tokens': 27}\n"
     ]
    }
   ],
   "source": [
    "response, content, token_dict = get_completion_and_token_count(\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "pzzDaUaL3R-J",
    "outputId": "0fe0ddaf-ed22-40cb-e87e-e495dada7978"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3923, 374, 220, 16, 10, 16, 30]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "encoding.encode(\"What is 1+1?\") # no. of input tokens counted by OpenAI API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "agJufpkQFo9W"
   },
   "source": [
    "# ***Chat API : LangChain***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WGB0fXFH3E2o"
   },
   "outputs": [],
   "source": [
    "# !pip install langchain --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-gUvYYDv8P7"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "M2EgnqTNFyBa"
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "v_p0WB9RF4Bc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amit.tewari\\AppData\\Roaming\\Python\\Python38\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = openai_api_key # MUST\n",
    "# To control the randomness and creativity of the generated text by an LLM, use temperature = 0.0\n",
    "chat = ChatOpenAI(temperature=0.0, model=llm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tmLmTqPawAq8"
   },
   "source": [
    "# Prompt template - Input into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "iClYMINJF6kt"
   },
   "outputs": [],
   "source": [
    "template_string = \"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \\\n",
    "into a style that is {style}. \\\n",
    "text: ```{text}```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "9J1DzQIhHprv"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "sktdvD3gHsJK",
    "outputId": "e276e826-8906-4fa2-8556-ad6b28815b67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['style', 'text'] template='Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\\n'\n",
      "['style', 'text']\n"
     ]
    }
   ],
   "source": [
    "print(prompt_template.messages[0].prompt)\n",
    "print(prompt_template.messages[0].prompt.input_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "46wd30wrHvQC",
    "outputId": "e98f1b8a-1868-4156-d759-c37774ef832a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "zIheocqYHy9R"
   },
   "outputs": [],
   "source": [
    "customer_email = \"\"\"\n",
    "Hi how are you doing\n",
    "\"\"\"\n",
    "customer_style = \"\"\"Spanish \\\n",
    "in a calm and respectful tone\n",
    "\"\"\"\n",
    "customer_messages = prompt_template.format_messages(\n",
    "                    style=customer_style,\n",
    "                    text=customer_email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "nx3RumyrIyYD",
    "outputId": "4c052b93-f0cc-4d2a-a548-b417f350f8b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[HumanMessage(content='Translate the text that is delimited by triple backticks into a style that is Spanish in a calm and respectful tone\\n. text: ```\\nHi how are you doing\\n```\\n')]\n",
      "<class 'langchain_core.messages.human.HumanMessage'>\n",
      "content='Translate the text that is delimited by triple backticks into a style that is Spanish in a calm and respectful tone\\n. text: ```\\nHi how are you doing\\n```\\n'\n"
     ]
    }
   ],
   "source": [
    "print(type(customer_messages))\n",
    "print(customer_messages)\n",
    "print(type(customer_messages[0]))\n",
    "print(customer_messages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "BOAGG3DNJEgr"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amit.tewari\\AppData\\Roaming\\Python\\Python38\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# Call the LLM to translate to the style of the customer message\n",
    "customer_response = chat(customer_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "zdZtpnVSJlQq",
    "outputId": "637fc9ce-9599-4312-e470-2aa3e94cb407"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hola, ¿cómo estás?\n"
     ]
    }
   ],
   "source": [
    "print(customer_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfD-LDiSwFec"
   },
   "source": [
    "# Parse the LLM output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "oDioHb8mdu9w"
   },
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "3eoyTsEiepwM"
   },
   "outputs": [],
   "source": [
    "email_body_schema = ResponseSchema(name=\"email_body\",\n",
    "                                      description=\"Extract email body\")\n",
    "\n",
    "email_from_schema = ResponseSchema(name=\"email_sender\",\n",
    "                                      description=\"Extract sender email\")\n",
    "\n",
    "email_from_name_schema = ResponseSchema(name=\"email_sender_name\",\n",
    "                                      description=\"Extract sender name\")\n",
    "\n",
    "email_from_designation_schema = ResponseSchema(name=\"email_sender_designation\",\n",
    "                                      description=\"Extract sender designation\")\n",
    "\n",
    "email_from_organization_schema = ResponseSchema(name=\"email_sender_organization\",\n",
    "                                      description=\"Extract sender orgaization\")\n",
    "\n",
    "email_date_schema = ResponseSchema(name=\"email_date\",\n",
    "                                       description=\"Extract email date\")\n",
    "\n",
    "email_to_schema = ResponseSchema(name=\"email_recipients\",\n",
    "                                      description=\"Extract comma separated email recipients\")\n",
    "\n",
    "response_schemas = [email_from_schema,\n",
    "                    email_from_name_schema,\n",
    "                    email_from_designation_schema,\n",
    "                    email_from_organization_schema,\n",
    "                    email_to_schema,\n",
    "                    email_date_schema,\n",
    "                    email_body_schema]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "6d5gBog4fwpx"
   },
   "outputs": [],
   "source": [
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N3jZZwxLf0Gk",
    "outputId": "1653f825-2aa6-4af3-d22f-0c946ad4aab1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"email_sender\": string  // Extract sender email\n",
      "\t\"email_sender_name\": string  // Extract sender name\n",
      "\t\"email_sender_designation\": string  // Extract sender designation\n",
      "\t\"email_sender_organization\": string  // Extract sender orgaization\n",
      "\t\"email_recipients\": string  // Extract comma separated email recipients\n",
      "\t\"email_date\": string  // Extract email date\n",
      "\t\"email_body\": string  // Extract email body\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "kJJ49a-cikiu"
   },
   "outputs": [],
   "source": [
    "email = \"From: Doe, John <JohnDoe@xyz.com> \\\n",
    "Sent: Wednesday, March 15, 2023 11:21 PM \\\n",
    "To: Hollester, Mike <@MHollester.com>; Parrera, Nicky <NParrera@xyz.com> \\\n",
    "Cc: Bush, Johnny <JBush@xyz.com>; Herding, Bill <BHerding@xyz.com> \\\n",
    "Subject: RE: Interested in AI product  \\\n",
    "I hope this email finds you well. \\\n",
    "I am reaching out to inquire about your company's AI offerings and services. \\\n",
    "Could you please provide more information regarding your capabilities and solutions in this field? \\\n",
    "Shawn Ponting \\\n",
    "Senior Product Manager \\\n",
    "Great Corporation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "rw_oJbLMhccE"
   },
   "outputs": [],
   "source": [
    "review_template_2 = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "email_body: Extract email body If this information is not found,output Unknown Email Body\n",
    "email_sender: Extract sender email If this information is not found, output Unknown Sender.\n",
    "email_sender_designation: Extract sender designation If this information is not found, output Unknown Sender Designation.\n",
    "email_sender_organization: Extract sender organization If this information is not found, output Unknown Sender Organization.\n",
    "email_date: Extract email date If this information is not found, output Unknown Date.\n",
    "email_recipients: Extract comma separated email recipients If this information is not found, output Unknown Recipients.\n",
    "\n",
    "text: {text}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "YsqGWylnf9ao"
   },
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(template=review_template_2)\n",
    "\n",
    "messages = prompt.format_messages(text=email,\n",
    "                                format_instructions=format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VeuIWYHgjCaU",
    "outputId": "1fe55f2d-da64-4e7e-9f75-2a05b17a0dad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the following text, extract the following information:\n",
      "\n",
      "email_body: Extract email body If this information is not found,output Unknown Email Body\n",
      "email_sender: Extract sender email If this information is not found, output Unknown Sender.\n",
      "email_sender_designation: Extract sender designation If this information is not found, output Unknown Sender Designation.\n",
      "email_sender_organization: Extract sender organization If this information is not found, output Unknown Sender Organization.\n",
      "email_date: Extract email date If this information is not found, output Unknown Date.\n",
      "email_recipients: Extract comma separated email recipients If this information is not found, output Unknown Recipients.\n",
      "\n",
      "text: From: Doe, John <JohnDoe@xyz.com> Sent: Wednesday, March 15, 2023 11:21 PM To: Hollester, Mike <@MHollester.com>; Parrera, Nicky <NParrera@xyz.com> Cc: Bush, Johnny <JBush@xyz.com>; Herding, Bill <BHerding@xyz.com> Subject: RE: Interested in AI product  I hope this email finds you well. I am reaching out to inquire about your company's AI offerings and services. Could you please provide more information regarding your capabilities and solutions in this field? Shawn Ponting Senior Product Manager Great Corporation\n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"email_sender\": string  // Extract sender email\n",
      "\t\"email_sender_name\": string  // Extract sender name\n",
      "\t\"email_sender_designation\": string  // Extract sender designation\n",
      "\t\"email_sender_organization\": string  // Extract sender orgaization\n",
      "\t\"email_recipients\": string  // Extract comma separated email recipients\n",
      "\t\"email_date\": string  // Extract email date\n",
      "\t\"email_body\": string  // Extract email body\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "O5ai56sajIRp"
   },
   "outputs": [],
   "source": [
    "response = chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4rESZeTFjKf-",
    "outputId": "60d59cc4-bed7-4584-fd08-7b2d8076dd46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\t\"email_sender\": \"JohnDoe@xyz.com\",\n",
      "\t\"email_sender_name\": \"Doe, John\",\n",
      "\t\"email_sender_designation\": \"Senior Product Manager\",\n",
      "\t\"email_sender_organization\": \"Great Corporation\",\n",
      "\t\"email_recipients\": \"Hollester, Mike <@MHollester.com>, Parrera, Nicky <NParrera@xyz.com>, Bush, Johnny <JBush@xyz.com>, Herding, Bill <BHerding@xyz.com>\",\n",
      "\t\"email_date\": \"Wednesday, March 15, 2023 11:21 PM\",\n",
      "\t\"email_body\": \"I hope this email finds you well. I am reaching out to inquire about your company's AI offerings and services. Could you please provide more information regarding your capabilities and solutions in this field?\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "VpYBofoBjNlk"
   },
   "outputs": [],
   "source": [
    "output_dict = output_parser.parse(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-ypJqzq1jPAn",
    "outputId": "be443cc4-9c7f-4ffa-9918-554a8c45ff43"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'email_sender': 'JohnDoe@xyz.com',\n",
       " 'email_sender_name': 'Doe, John',\n",
       " 'email_sender_designation': 'Senior Product Manager',\n",
       " 'email_sender_organization': 'Great Corporation',\n",
       " 'email_recipients': 'Hollester, Mike <@MHollester.com>, Parrera, Nicky <NParrera@xyz.com>, Bush, Johnny <JBush@xyz.com>, Herding, Bill <BHerding@xyz.com>',\n",
       " 'email_date': 'Wednesday, March 15, 2023 11:21 PM',\n",
       " 'email_body': \"I hope this email finds you well. I am reaching out to inquire about your company's AI offerings and services. Could you please provide more information regarding your capabilities and solutions in this field?\"}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "glkloAdBqxmq"
   },
   "source": [
    "# LangChain: Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory of history in a chat keeps on appending by Langchain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, this is a LangChain tutorial to explain Memory for chat application\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello! I'm excited to help you understand Memory for chat applications. Memory in this context refers to the ability of the chat application to remember past interactions and information shared between users. This can include things like chat history, user preferences, and personalized recommendations based on previous conversations. Is there a specific aspect of Memory for chat applications that you would like me to explain further?\""
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, this is a LangChain tutorial to explain Memory for chat application\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, this is a LangChain tutorial to explain Memory for chat application\n",
      "AI: Hello! I'm excited to help you understand Memory for chat applications. Memory in this context refers to the ability of the chat application to remember past interactions and information shared between users. This can include things like chat history, user preferences, and personalized recommendations based on previous conversations. Is there a specific aspect of Memory for chat applications that you would like me to explain further?\n",
      "Human: I will use simple examples to explain Langchain Memory \n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"That sounds like a great approach! Using simple examples can really help clarify how Memory works in LangChain. Feel free to provide any examples or scenarios you have in mind, and I'll do my best to explain how Memory would be utilized in those situations. Let's dive into it!\""
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"I will use simple examples to explain Langchain Memory \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, this is a LangChain tutorial to explain Memory for chat application\n",
      "AI: Hello! I'm excited to help you understand Memory for chat applications. Memory in this context refers to the ability of the chat application to remember past interactions and information shared between users. This can include things like chat history, user preferences, and personalized recommendations based on previous conversations. Is there a specific aspect of Memory for chat applications that you would like me to explain further?\n",
      "Human: I will use simple examples to explain Langchain Memory \n",
      "AI: That sounds like a great approach! Using simple examples can really help clarify how Memory works in LangChain. Feel free to provide any examples or scenarios you have in mind, and I'll do my best to explain how Memory would be utilized in those situations. Let's dive into it!\n",
      "Human: What are we going to learn in this tutorial?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"In this tutorial, we will learn about how Memory is used in LangChain to enhance chat applications. We will explore how Memory can store and retrieve past interactions, user preferences, and personalized recommendations. Additionally, we will discuss how Memory can improve user experience and make chat applications more engaging and efficient. By the end of this tutorial, you will have a better understanding of how Memory works in LangChain and how it can benefit chat applications. Let's get started!\""
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What are we going to learn in this tutorial?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi, this is a LangChain tutorial to explain Memory for chat application\n",
      "AI: Hello! I'm excited to help you understand Memory for chat applications. Memory in this context refers to the ability of the chat application to remember past interactions and information shared between users. This can include things like chat history, user preferences, and personalized recommendations based on previous conversations. Is there a specific aspect of Memory for chat applications that you would like me to explain further?\n",
      "Human: I will use simple examples to explain Langchain Memory \n",
      "AI: That sounds like a great approach! Using simple examples can really help clarify how Memory works in LangChain. Feel free to provide any examples or scenarios you have in mind, and I'll do my best to explain how Memory would be utilized in those situations. Let's dive into it!\n",
      "Human: What are we going to learn in this tutorial?\n",
      "AI: In this tutorial, we will learn about how Memory is used in LangChain to enhance chat applications. We will explore how Memory can store and retrieve past interactions, user preferences, and personalized recommendations. Additionally, we will discuss how Memory can improve user experience and make chat applications more engaging and efficient. By the end of this tutorial, you will have a better understanding of how Memory works in LangChain and how it can benefit chat applications. Let's get started!\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer) # History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide additional context \n",
    "today_date = str(datetime.date.today())\n",
    "memory.save_context({\"input\": \"What is the date today?\" }, \n",
    "                    {\"output\": today_date}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi, this is a LangChain tutorial to explain Memory for chat application\\nAI: Hello! I'm excited to help you understand Memory for chat applications. Memory in this context refers to the ability of the chat application to remember past interactions and information shared between users. This can include things like chat history, user preferences, and personalized recommendations based on previous conversations. Is there a specific aspect of Memory for chat applications that you would like me to explain further?\\nHuman: I will use simple examples to explain Langchain Memory \\nAI: That sounds like a great approach! Using simple examples can really help clarify how Memory works in LangChain. Feel free to provide any examples or scenarios you have in mind, and I'll do my best to explain how Memory would be utilized in those situations. Let's dive into it!\\nHuman: What are we going to learn in this tutorial?\\nAI: In this tutorial, we will learn about how Memory is used in LangChain to enhance chat applications. We will explore how Memory can store and retrieve past interactions, user preferences, and personalized recommendations. Additionally, we will discuss how Memory can improve user experience and make chat applications more engaging and efficient. By the end of this tutorial, you will have a better understanding of how Memory works in LangChain and how it can benefit chat applications. Let's get started!\\nHuman: What is the date today?\\nAI: 2024-03-16\\nHuman: What is the date today?\\nAI: 2024-03-16\"}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember just 'LAST K converstaions' instead of entire history to avoid sending entire conversation to OpenAI API\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "memory = ConversationBufferWindowMemory(k=1)               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide additional context \n",
    "today_date = str(datetime.date.today())\n",
    "memory.save_context({\"input\": \"What is your City of residenc?\" }, \n",
    "                    {\"output\": \"Delhi\"}) \n",
    "memory.save_context({\"input\": \"What is your Country of residence?\" }, \n",
    "                    {\"output\": \"India\"}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: What is your Country of residence?\\nAI: India'}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "# memory =  ConversationBufferWindowMemory(k=1)  # usage\n",
    "# conversation = ConversationChain(\n",
    "#     llm=llm, \n",
    "#     memory = memory,\n",
    "#     verbose=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit the # of tokens to send instead of entire history\n",
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.llms import OpenAI\n",
    "llm = ChatOpenAI(temperature=0.0, model=llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=20)\n",
    "memory.save_context({\"input\": \"What is your City of residence?\" }, \n",
    "                    {\"output\": \"Delhi\"}) \n",
    "memory.save_context({\"input\": \"What is your Country of residence?\" }, \n",
    "                    {\"output\": \"India\"})\n",
    "memory.save_context({\"input\": \"Which city is your office located?\" }, \n",
    "                    {\"output\": \"New York\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'AI: New York'}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of limiting fixed # of token, lets use LLM to write the summary of conversation so far and remem the summary\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=30) \n",
    "# entire hostory so far will be summarized in max 30 token \n",
    "memory.save_context({\"input\": \"What is your City of residence?\" }, \n",
    "                    {\"output\": \"Delhi\"}) \n",
    "memory.save_context({\"input\": \"What is your Country of residence?\" }, \n",
    "                    {\"output\": \"India\"})\n",
    "memory.save_context({\"input\": \"Which city is your office located?\" }, \n",
    "                    {\"output\": \"New York\"})\n",
    "memory.save_context({\"input\": \"What is the date today?\" }, \n",
    "                    {\"output\": today_date}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'System: The human asks the AI\\'s City of residence, to which the AI responds \"Delhi.\" When asked for the AI\\'s Country of residence, the AI replies \"India.\" The human then inquires about the city where the AI\\'s office is located, and the AI answers \"New York.\"\\nHuman: What is the date today?\\nAI: 2024-03-16'}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human asks the AI's City of residence, to which the AI responds \"Delhi.\" When asked for the AI's Country of residence, the AI replies \"India.\" The human then inquires about the city where the AI's office is located, and the AI answers \"New York.\" When asked about the date, the AI responds \"2024-03-16.\"\n",
      "Human: What is the office location?\n",
      "AI: The office location is in Manhattan, New York City.\n",
      "Human: What is the office location city?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The office location city is New York City.'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q/A or chat using history/memory \n",
    "conversation.predict(input=\"What is the office location city?\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'img1.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-119-75f823412b9d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mImage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'img1.png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\display.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, url, filename, format, embed, width, height, retina, unconfined, metadata)\u001b[0m\n\u001b[0;32m   1222\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretina\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mretina\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munconfined\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munconfined\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1224\u001b[1;33m         super(Image, self).__init__(data=data, url=url, filename=filename, \n\u001b[0m\u001b[0;32m   1225\u001b[0m                 metadata=metadata)\n\u001b[0;32m   1226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\display.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, url, filename, metadata)\u001b[0m\n\u001b[0;32m    628\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 630\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    631\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    632\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\display.py\u001b[0m in \u001b[0;36mreload\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1254\u001b[0m         \u001b[1;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1256\u001b[1;33m             \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1257\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretina\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1258\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_retina_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\display.py\u001b[0m in \u001b[0;36mreload\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    653\u001b[0m         \u001b[1;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    654\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_flags\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murl\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'img1.png'"
     ]
    }
   ],
   "source": [
    "Image(filename='img1.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
